{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f0319c7-5bc4-4242-be4b-c809bb1612f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b3d923f-a482-419b-a9c1-cb7cd8b6cdc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "All the Transformations are available here https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations\n",
    "and https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html#pyspark.RDD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b305111a-7500-4daf-bd4e-d2b5221c3bbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### map\n",
    "Return a new distributed dataset formed by passing each element of the source through a function func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5145aaee-293b-47f2-8414-5b2649b7d297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[1]: [('a', 1), ('b', 1), ('c', 1)]"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
    "sorted(rdd.map(lambda x: (x, 1)).collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf249ff8-e472-4089-b1e2-0598fd25ba69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### mapPartitions\n",
    "\n",
    "Similar to map, but runs separately on each partition (block) of the RDD, so func must be of type Iterator<T> => Iterator<U> when running on an RDD of type T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8636618-deae-49f8-a312-70c169c20553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: [11, 18]"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 8, 2, 3, 4, 5, 6], 2)\n",
    "\n",
    "def f(iterator): \n",
    "    yield sum(iterator)\n",
    "\n",
    "rdd.mapPartitions(f).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0299640f-d773-47f7-b6a3-1b24c603620b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: [2, 16, 4, 6, 8, 10, 12]"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 8, 2, 3, 4, 5, 6], 2)\n",
    "\n",
    "def f(iterator): \n",
    "    for i in iterator:\n",
    "        yield i*2\n",
    "\n",
    "rdd.mapPartitions(f).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d3640ec-6b5b-4b06-bffe-4843cd67d31c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### mapPartitionsWithIndex\n",
    "\n",
    "Similar to mapPartitions, but also provides func with an integer value representing the index of the partition, so func must be of type (Int, Iterator<T>) => Iterator<U> when running on an RDD of type T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8a20020-2bc7-4d93-82d3-2b78cc618409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: ['data = 1 and index = 0',\n 'data = 2 and index = 1',\n 'data = 3 and index = 2',\n 'data = 4 and index = 3']"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
    "def f(splitIndex, iterator): \n",
    "    data = 0\n",
    "    for i in iterator:\n",
    "        data += i\n",
    "    yield (f'data = {data} and index = {splitIndex}')\n",
    "\n",
    "\n",
    "rdd.mapPartitionsWithIndex(f).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4fc4d66-8f54-4c8a-afa8-5e8cad964788",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: ['data = 3 and index = 0', 'data = 7 and index = 1']"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
    "def f(splitIndex, iterator): \n",
    "    data = 0\n",
    "    for i in iterator:\n",
    "        data += i\n",
    "    yield (f'data = {data} and index = {splitIndex}')\n",
    "\n",
    "\n",
    "rdd.mapPartitionsWithIndex(f).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "575ae770-b88c-4aca-9145-5ab2c16e2023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### flatMap\n",
    "\n",
    "Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2353079-372e-4c70-a1f2-0544c1debb44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: [1, 1, 2, 1, 2, 3]"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([2, 3, 4])\n",
    "\n",
    "rdd.flatMap(lambda x: range(1, x)).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4d52aae-cd81-43d8-bd2f-ce20815661e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]"
     ]
    }
   ],
   "source": [
    "rdd.flatMap(lambda x: [(x, x), (x, x)]).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd7998ec-0bf8-4fcc-9076-3f077cc947a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### flatMapValues\n",
    "\n",
    "Pass each value in the key-value pair RDD through a flatMap function without changing the keys; this also retains the original RDD’s partitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c729c538-8243-4dab-b3f0-807266f98466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: [('a', 'x'), ('a', ['y', 'z']), ('b', 'p'), ('b', 'r')]"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", [\"x\", [\"y\", \"z\"]]), (\"b\", [\"p\", \"r\"])])\n",
    "\n",
    "def f(x): \n",
    "    return x\n",
    "    \n",
    "rdd.flatMapValues(f).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dbb5c8b-482f-4f8f-9a21-3f8310da3a08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]"
     ]
    }
   ],
   "source": [
    "rdd.flatMapValues(f).flatMapValues(f).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77d30258-7b0a-4a4b-9a0d-38b34430dad9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### mapValues\n",
    "\n",
    "Pass each value in the key-value pair RDD through a map function without changing the keys; this also retains the original RDD’s partitioning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff96917f-dd45-45b3-88d4-c4b453a56831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: [('a', 3), ('b', 1)]"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n",
    "def f(x): \n",
    "    return len(x)\n",
    "\n",
    "rdd.mapValues(f).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "969fd28b-46df-4764-9ca5-db1e9baf5485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### filter\n",
    "\n",
    "Return a new dataset formed by selecting those elements of the source on which func returns true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "136c2e38-0f46-4bac-a3ca-7be975c7e490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: [2, 4]"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "rdd.filter(lambda x: x % 2 == 0).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca45be74-b88d-4e0b-a69c-901ecde834ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### sample\n",
    "\n",
    "Sample a fraction fraction of the data, with or without replacement, using a given random number generator seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "939a45d3-2bb1-4ec3-ba38-78d90e9eee1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: [4, 26, 39, 41, 42, 52, 63, 76, 80, 86, 97]"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(100), 4)\n",
    "rdd.sample(False, 0.1, 81).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6ec6176-5707-4f27-96fa-462fa8766fc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: [('a', 0),\n ('a', 1),\n ('a', 2),\n ('a', 3),\n ('a', 4),\n ('b', 0),\n ('b', 1),\n ('b', 2),\n ('b', 3),\n ('b', 4)]"
     ]
    }
   ],
   "source": [
    "fractions = {\"a\": 0.2, \"b\": 0.1}\n",
    "rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 5)))\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62401454-6687-447d-9161-3c363a90838c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: [('a', 1), ('a', 2), ('b', 4)]"
     ]
    }
   ],
   "source": [
    "rdd.sampleByKey(False, fractions, 2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd6af5e4-f921-4caa-bfcd-b04e745255f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### union\n",
    "\n",
    "Return a new dataset that contains the union of the elements in the source dataset and the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de135b78-4765-4c2e-8bc6-13b88948abf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: [1, 1, 2, 3, 1, 1, 2, 3]"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 1, 2, 3])\n",
    "rdd.union(rdd).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1cdd663-83e0-4476-ac8d-eb675779dab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### intersection \n",
    "\n",
    "Return a new RDD that contains the intersection of elements in the source dataset and the argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "581de8f9-5676-4416-bfa2-ae6a97a71340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[16]: [1, 2, 3]"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
    "rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
    "rdd1.intersection(rdd2).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c8b72d1-966a-4cbc-9e69-10485d1e435f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### distinct\n",
    "\n",
    "Return a new dataset that contains the distinct elements of the source dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55b382e6-0d4e-4201-8d3f-cfaef6413bd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: [1, 2, 3]"
     ]
    }
   ],
   "source": [
    "sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c100eebf-d510-4dca-bf90-275c13f4b2ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### repartition\n",
    "Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0602a386-edfb-4b74-8850-6c1761f26bb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[18]: [[1], [2, 3], [4, 5], [6, 7]]"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
    "sorted(rdd.glom().collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76265d05-a635-498e-a750-715492313bf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[19]: [[4, 5, 6, 7], [1, 2, 3]]"
     ]
    }
   ],
   "source": [
    "\n",
    "rdd.repartition(2).glom().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7103a944-f7b6-452c-b688-fa4555907a37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[20]: [[], [], [], [6, 7], [1, 2, 3, 4, 5]]"
     ]
    }
   ],
   "source": [
    "\n",
    "rdd.repartition(5).glom().collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b86d8653-8ef8-46d5-92b1-902c78cbda28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### repartitionAndSortWithinPartitions\n",
    "\n",
    "Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys. This is more efficient than calling repartition and then sorting within each partition because it can push the sorting down into the shuffle machinery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3640ba5-304b-4468-8b58-0931c29dac20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[21]: [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n",
    "rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\n",
    "rdd2.glom().collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a953845c-f237-4910-aec6-674180733ca6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### groupByKey\n",
    "When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs.\n",
    "\n",
    "Note: If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will yield much better performance.\n",
    "\n",
    "Note: By default, the level of parallelism in the output depends on the number of partitions of the parent RDD. You can pass an optional numPartitions argument to set a different number of tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73113a1b-294d-4499-bc8c-f9fd8032e859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[22]: [('a', <pyspark.resultiterable.ResultIterable at 0x7f9a10e551f0>),\n ('b', <pyspark.resultiterable.ResultIterable at 0x7f9a10dfd0d0>)]"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 4)])\n",
    "rdd.groupByKey().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "559f49e8-3dbc-4b09-8bd7-e1aa1123ef47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[23]: [1, 4]"
     ]
    }
   ],
   "source": [
    "list(rdd.groupByKey().collect()[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f0d38a6-ceb7-4a77-b8a2-a95eb70d39e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[24]: [('a', 2), ('b', 1)]"
     ]
    }
   ],
   "source": [
    "rdd.groupByKey().mapValues(len).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f4ac672-88ec-48ca-957f-ed7a21c622d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[25]: [('a', [1, 4]), ('b', [1])]"
     ]
    }
   ],
   "source": [
    "\n",
    "rdd.groupByKey().mapValues(list).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d13b9295-9593-488e-af03-9b05bb24bd9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### reduceByKey\n",
    "\n",
    "When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4cbb39b-8843-4672-bdfa-ae037dc84de4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[26]: [('a', 5), ('b', 1)]"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 4)])\n",
    "sorted(rdd.reduceByKey(add).collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c498ff4f-2042-47c4-a32a-5e830c83a178",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### aggregateByKey\n",
    "\n",
    "When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral \"zero\" value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "239f5eb6-1b6d-4ad9-8dc1-49085868fa9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[27]: [('a', (9, 5)), ('b', (5, 2))]"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2), (\"a\", 2), (\"a\", 2), (\"a\", 2), (\"b\", 4)])\n",
    "seqFunc = (lambda x, y: (x[0] + y, x[1] + 1))\n",
    "combFunc = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "rdd.aggregateByKey((0, 0), seqFunc, combFunc).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7acbb7c-2682-4fbe-82ad-8855dc3ff759",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### sortByKey\n",
    "\n",
    "When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean ascending argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44ca1ff4-21bd-4356-92ce-d23ae0bcb86b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[28]: ('1', 3)"
     ]
    }
   ],
   "source": [
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "sc.parallelize(tmp).sortByKey().first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "854a0d5c-6cde-46f7-914b-ec5339f0547b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[29]: [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]"
     ]
    }
   ],
   "source": [
    "\n",
    "sc.parallelize(tmp).sortByKey(True, 1).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dd2e05a-bd12-4378-b0b5-42e9d4d22669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[30]: [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]"
     ]
    }
   ],
   "source": [
    "\n",
    "sc.parallelize(tmp).sortByKey(True, 2).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eba60cc-5cfc-451d-909e-fe19684f63b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[31]: [('a', 3),\n ('fleece', 7),\n ('had', 2),\n ('lamb', 5),\n ('little', 4),\n ('Mary', 1),\n ('was', 8),\n ('white', 9),\n ('whose', 6)]"
     ]
    }
   ],
   "source": [
    "\n",
    "tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
    "tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
    "sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7285176e-2c61-4ef3-b12a-4960c2dd0ef1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### join\n",
    "\n",
    "When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecf9025a-3a12-40aa-8b29-08569180189d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[32]: [('a', (1, 2)), ('a', (1, 3))]"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "rdd2 = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
    "sorted(rdd1.join(rdd2).collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "369d5c18-f639-4aa7-a405-5d52e9a43e2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### cogroup\n",
    "\n",
    "When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable<V>, Iterable<W>)) tuples. This operation is also called groupWith.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45769cb6-85f0-4f9f-bf2e-57aea24e633e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[33]: [('b',\n  (<pyspark.resultiterable.ResultIterable at 0x7f9a10e69f70>,\n   <pyspark.resultiterable.ResultIterable at 0x7f9a10e70b50>)),\n ('c',\n  (<pyspark.resultiterable.ResultIterable at 0x7f9a10e70910>,\n   <pyspark.resultiterable.ResultIterable at 0x7f9a10e70df0>)),\n ('a',\n  (<pyspark.resultiterable.ResultIterable at 0x7f9a10e70ac0>,\n   <pyspark.resultiterable.ResultIterable at 0x7f9a10e70be0>))]"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "rdd2 = sc.parallelize([(\"a\", 2), (\"a\", 5), (\"b\", 6), (\"c\", 7)])\n",
    "rdd1.cogroup(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89e78328-7756-4a9f-bff1-a43f890101bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[34]: [('b',\n  (<pyspark.resultiterable.ResultIterable at 0x7f9a10e04730>,\n   <pyspark.resultiterable.ResultIterable at 0x7f9a10e04b50>)),\n ('c',\n  (<pyspark.resultiterable.ResultIterable at 0x7f9a10e04820>,\n   <pyspark.resultiterable.ResultIterable at 0x7f9a10e042b0>)),\n ('a',\n  (<pyspark.resultiterable.ResultIterable at 0x7f9a10e047c0>,\n   <pyspark.resultiterable.ResultIterable at 0x7f9a10e04df0>))]"
     ]
    }
   ],
   "source": [
    "list(rdd1.cogroup(rdd2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bccb33c6-283e-4b78-83ad-1cd322c36d0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[35]: [('a', ([1], [2, 5])), ('b', ([4], [6])), ('c', ([], [7]))]"
     ]
    }
   ],
   "source": [
    "[(x, tuple(map(list, y))) for x, y in sorted(list(rdd1.cogroup(rdd2).collect()))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e462a92-5554-4beb-906e-71deebff0639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### cartesian\n",
    "\n",
    "When called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "438daf18-0b61-456a-bf6b-981a4ac4e2e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[36]: [(1, 1), (1, 2), (2, 1), (2, 2)]"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2])\n",
    "rdd.cartesian(rdd).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f30458a8-f448-42c6-aeac-0a17f4793576",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### pipe\n",
    "\n",
    "Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the process's stdin and lines output to its stdout are returned as an RDD of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6255d292-f0f6-457d-952a-fce57b3197c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[37]: ['1', '1', '1', '1', '1', '1', '1', '1']"
     ]
    }
   ],
   "source": [
    "sc.parallelize(['1', '2', '', '3']).pipe('echo 1').collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b77f55a9-5aa4-499a-b206-a97395ccacee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### coalesce\n",
    "\n",
    "Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently after filtering down a large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04079a61-830d-4712-a524-c203afffaed8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[38]: [[1], [2, 3], [4, 5]]"
     ]
    }
   ],
   "source": [
    "sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e8352bf-d7e3-4177-a576-c58ccc3ba237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[39]: [[1, 2, 3, 4, 5]]"
     ]
    }
   ],
   "source": [
    "\n",
    "sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24618923-ad92-4c6a-972f-3d69407451c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3. RDD Transformations and Actions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
