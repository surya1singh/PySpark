{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2007d570-7c39-4bbe-affd-0e66cc2c9e35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Read Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b46cc23c-6945-4a83-85ce-5c423b3b3b8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- VendorID: long (nullable = true)\n |-- tpep_pickup_datetime: timestamp (nullable = true)\n |-- tpep_dropoff_datetime: timestamp (nullable = true)\n |-- passenger_count: double (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- RatecodeID: double (nullable = true)\n |-- store_and_fwd_flag: string (nullable = true)\n |-- PULocationID: long (nullable = true)\n |-- DOLocationID: long (nullable = true)\n |-- payment_type: long (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- extra: double (nullable = true)\n |-- mta_tax: double (nullable = true)\n |-- tip_amount: double (nullable = true)\n |-- tolls_amount: double (nullable = true)\n |-- improvement_surcharge: double (nullable = true)\n |-- total_amount: double (nullable = true)\n |-- congestion_surcharge: double (nullable = true)\n |-- airport_fee: double (nullable = true)\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>VendorID</th><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>RatecodeID</th><th>store_and_fwd_flag</th><th>PULocationID</th><th>DOLocationID</th><th>payment_type</th><th>fare_amount</th><th>extra</th><th>mta_tax</th><th>tip_amount</th><th>tolls_amount</th><th>improvement_surcharge</th><th>total_amount</th><th>congestion_surcharge</th><th>airport_fee</th></tr></thead><tbody><tr><td>1</td><td>2021-01-01T00:30:10.000+0000</td><td>2021-01-01T00:36:12.000+0000</td><td>1.0</td><td>2.1</td><td>1.0</td><td>N</td><td>142</td><td>43</td><td>2</td><td>8.0</td><td>3.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>11.8</td><td>2.5</td><td>null</td></tr><tr><td>1</td><td>2021-01-01T00:51:20.000+0000</td><td>2021-01-01T00:52:19.000+0000</td><td>1.0</td><td>0.2</td><td>1.0</td><td>N</td><td>238</td><td>151</td><td>2</td><td>3.0</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>4.3</td><td>0.0</td><td>null</td></tr><tr><td>1</td><td>2021-01-01T00:43:30.000+0000</td><td>2021-01-01T01:11:06.000+0000</td><td>1.0</td><td>14.7</td><td>1.0</td><td>N</td><td>132</td><td>165</td><td>1</td><td>42.0</td><td>0.5</td><td>0.5</td><td>8.65</td><td>0.0</td><td>0.3</td><td>51.95</td><td>0.0</td><td>null</td></tr><tr><td>1</td><td>2021-01-01T00:15:48.000+0000</td><td>2021-01-01T00:31:01.000+0000</td><td>0.0</td><td>10.6</td><td>1.0</td><td>N</td><td>138</td><td>132</td><td>1</td><td>29.0</td><td>0.5</td><td>0.5</td><td>6.05</td><td>0.0</td><td>0.3</td><td>36.35</td><td>0.0</td><td>null</td></tr><tr><td>2</td><td>2021-01-01T00:31:49.000+0000</td><td>2021-01-01T00:48:21.000+0000</td><td>1.0</td><td>4.94</td><td>1.0</td><td>N</td><td>68</td><td>33</td><td>1</td><td>16.5</td><td>0.5</td><td>0.5</td><td>4.06</td><td>0.0</td><td>0.3</td><td>24.36</td><td>2.5</td><td>null</td></tr><tr><td>1</td><td>2021-01-01T00:16:29.000+0000</td><td>2021-01-01T00:24:30.000+0000</td><td>1.0</td><td>1.6</td><td>1.0</td><td>N</td><td>224</td><td>68</td><td>1</td><td>8.0</td><td>3.0</td><td>0.5</td><td>2.35</td><td>0.0</td><td>0.3</td><td>14.15</td><td>2.5</td><td>null</td></tr><tr><td>1</td><td>2021-01-01T00:00:28.000+0000</td><td>2021-01-01T00:17:28.000+0000</td><td>1.0</td><td>4.1</td><td>1.0</td><td>N</td><td>95</td><td>157</td><td>2</td><td>16.0</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>17.3</td><td>0.0</td><td>null</td></tr><tr><td>1</td><td>2021-01-01T00:12:29.000+0000</td><td>2021-01-01T00:30:34.000+0000</td><td>1.0</td><td>5.7</td><td>1.0</td><td>N</td><td>90</td><td>40</td><td>2</td><td>18.0</td><td>3.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>21.8</td><td>2.5</td><td>null</td></tr><tr><td>1</td><td>2021-01-01T00:39:16.000+0000</td><td>2021-01-01T01:00:13.000+0000</td><td>1.0</td><td>9.1</td><td>1.0</td><td>N</td><td>97</td><td>129</td><td>4</td><td>27.5</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>28.8</td><td>0.0</td><td>null</td></tr><tr><td>1</td><td>2021-01-01T00:26:12.000+0000</td><td>2021-01-01T00:39:46.000+0000</td><td>2.0</td><td>2.7</td><td>1.0</td><td>N</td><td>263</td><td>142</td><td>1</td><td>12.0</td><td>3.0</td><td>0.5</td><td>3.15</td><td>0.0</td><td>0.3</td><td>18.95</td><td>2.5</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "2021-01-01T00:30:10.000+0000",
         "2021-01-01T00:36:12.000+0000",
         1.0,
         2.1,
         1.0,
         "N",
         142,
         43,
         2,
         8.0,
         3.0,
         0.5,
         0.0,
         0.0,
         0.3,
         11.8,
         2.5,
         null
        ],
        [
         1,
         "2021-01-01T00:51:20.000+0000",
         "2021-01-01T00:52:19.000+0000",
         1.0,
         0.2,
         1.0,
         "N",
         238,
         151,
         2,
         3.0,
         0.5,
         0.5,
         0.0,
         0.0,
         0.3,
         4.3,
         0.0,
         null
        ],
        [
         1,
         "2021-01-01T00:43:30.000+0000",
         "2021-01-01T01:11:06.000+0000",
         1.0,
         14.7,
         1.0,
         "N",
         132,
         165,
         1,
         42.0,
         0.5,
         0.5,
         8.65,
         0.0,
         0.3,
         51.95,
         0.0,
         null
        ],
        [
         1,
         "2021-01-01T00:15:48.000+0000",
         "2021-01-01T00:31:01.000+0000",
         0.0,
         10.6,
         1.0,
         "N",
         138,
         132,
         1,
         29.0,
         0.5,
         0.5,
         6.05,
         0.0,
         0.3,
         36.35,
         0.0,
         null
        ],
        [
         2,
         "2021-01-01T00:31:49.000+0000",
         "2021-01-01T00:48:21.000+0000",
         1.0,
         4.94,
         1.0,
         "N",
         68,
         33,
         1,
         16.5,
         0.5,
         0.5,
         4.06,
         0.0,
         0.3,
         24.36,
         2.5,
         null
        ],
        [
         1,
         "2021-01-01T00:16:29.000+0000",
         "2021-01-01T00:24:30.000+0000",
         1.0,
         1.6,
         1.0,
         "N",
         224,
         68,
         1,
         8.0,
         3.0,
         0.5,
         2.35,
         0.0,
         0.3,
         14.15,
         2.5,
         null
        ],
        [
         1,
         "2021-01-01T00:00:28.000+0000",
         "2021-01-01T00:17:28.000+0000",
         1.0,
         4.1,
         1.0,
         "N",
         95,
         157,
         2,
         16.0,
         0.5,
         0.5,
         0.0,
         0.0,
         0.3,
         17.3,
         0.0,
         null
        ],
        [
         1,
         "2021-01-01T00:12:29.000+0000",
         "2021-01-01T00:30:34.000+0000",
         1.0,
         5.7,
         1.0,
         "N",
         90,
         40,
         2,
         18.0,
         3.0,
         0.5,
         0.0,
         0.0,
         0.3,
         21.8,
         2.5,
         null
        ],
        [
         1,
         "2021-01-01T00:39:16.000+0000",
         "2021-01-01T01:00:13.000+0000",
         1.0,
         9.1,
         1.0,
         "N",
         97,
         129,
         4,
         27.5,
         0.5,
         0.5,
         0.0,
         0.0,
         0.3,
         28.8,
         0.0,
         null
        ],
        [
         1,
         "2021-01-01T00:26:12.000+0000",
         "2021-01-01T00:39:46.000+0000",
         2.0,
         2.7,
         1.0,
         "N",
         263,
         142,
         1,
         12.0,
         3.0,
         0.5,
         3.15,
         0.0,
         0.3,
         18.95,
         2.5,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "VendorID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "tpep_pickup_datetime",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "tpep_dropoff_datetime",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "passenger_count",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "trip_distance",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "RatecodeID",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "store_and_fwd_flag",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "PULocationID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DOLocationID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "payment_type",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "fare_amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "extra",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "mta_tax",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "tip_amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "tolls_amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "improvement_surcharge",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "total_amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "congestion_surcharge",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "airport_fee",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_location = \"/FileStore/tables/yellow_tripdata_2021_01.parquet\"\n",
    "file_type = \"parquet\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"false\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df = spark.read.parquet(file_location) \n",
    "df.printSchema()\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "baba5b9f-8f9e-4b8c-bd03-593184c4a27f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Read Json File to DataFrame\n",
    "\n",
    "Get data from https://github.com/indiacloudtv/pyspark_on_google_colab/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54a5c6ad-2d07-420b-b8cc-ad258182fcda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>user_city</th><th>user_id</th><th>user_name</th></tr></thead><tbody><tr><td>London</td><td>1</td><td>John</td></tr><tr><td>New York</td><td>2</td><td>Martin</td></tr><tr><td>Sydney</td><td>3</td><td>Sam</td></tr><tr><td>Mexico City</td><td>4</td><td>Alan</td></tr><tr><td>Florida</td><td>5</td><td>Jacob</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "London",
         1,
         "John"
        ],
        [
         "New York",
         2,
         "Martin"
        ],
        [
         "Sydney",
         3,
         "Sam"
        ],
        [
         "Mexico City",
         4,
         "Alan"
        ],
        [
         "Florida",
         5,
         "Jacob"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "user_city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "user_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "user_name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = \"\"\"\n",
    "{\"user_id\":1,\"user_name\":\"John\",\"user_city\":\"London\"}\n",
    "{\"user_id\":2,\"user_name\":\"Martin\",\"user_city\":\"New York\"}\n",
    "{\"user_id\":3,\"user_name\":\"Sam\",\"user_city\":\"Sydney\"}\n",
    "{\"user_id\":4,\"user_name\":\"Alan\",\"user_city\":\"Mexico City\"}\n",
    "{\"user_id\":5,\"user_name\":\"Jacob\",\"user_city\":\"Florida\"}\n",
    "\"\"\"\n",
    "\n",
    "file_location = \"/FileStore/tables/user_detail.json\"\n",
    "file_type = \"json\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"false\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4dac61a-cb70-45df-a8bc-ffbc34e0fbd9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "But sometime json is in multiline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd1a8b59-527a-42a9-a72b-2c07ef25024a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3187913262587314>:31\u001B[0m\n",
       "\u001B[1;32m     29\u001B[0m json_file_path_2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/FileStore/tables/user_detail_multiline.json\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     30\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mjson(path\u001B[38;5;241m=\u001B[39mjson_file_path_2)\n",
       "\u001B[0;32m---> 31\u001B[0m display(df)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/display.py:83\u001B[0m, in \u001B[0;36mDisplay.display\u001B[0;34m(self, input, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     80\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrigger\u001B[39m\u001B[38;5;124m'\u001B[39m):\n",
       "\u001B[1;32m     81\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTriggers can only be set for streaming queries.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[0;32m---> 83\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_custom_display_data\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     85\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mlist\u001B[39m):\n",
       "\u001B[1;32m     86\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession\u001B[38;5;241m.\u001B[39mcreateDataFrame(\u001B[38;5;28minput\u001B[39m))\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/display.py:36\u001B[0m, in \u001B[0;36mDisplay.add_custom_display_data\u001B[0;34m(self, data_type, data)\u001B[0m\n",
       "\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21madd_custom_display_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, data_type, data):\n",
       "\u001B[1;32m     35\u001B[0m     custom_display_key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(uuid\u001B[38;5;241m.\u001B[39muuid4())\n",
       "\u001B[0;32m---> 36\u001B[0m     return_code \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mentry_point\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maddCustomDisplayData\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcustom_display_key\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     37\u001B[0m     ip_display({\n",
       "\u001B[1;32m     38\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/vnd.databricks.v1+display\u001B[39m\u001B[38;5;124m\"\u001B[39m: custom_display_key,\n",
       "\u001B[1;32m     39\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext/plain\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<Databricks Output (not supported in output widgets)>\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     40\u001B[0m     },\n",
       "\u001B[1;32m     41\u001B[0m                raw\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\u001B[1;32m     42\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m return_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\n",
       "referenced columns only include the internal corrupt record column\n",
       "(named _corrupt_record by default). For example:\n",
       "spark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\n",
       "and spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\n",
       "Instead, you can cache or save the parsed results and then send the same query.\n",
       "For example, val df = spark.read.schema(schema).csv(file).cache() and then\n",
       "df.filter($\"_corrupt_record\".isNotNull).count()."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-3187913262587314>:31\u001B[0m\n\u001B[1;32m     29\u001B[0m json_file_path_2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/FileStore/tables/user_detail_multiline.json\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     30\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mjson(path\u001B[38;5;241m=\u001B[39mjson_file_path_2)\n\u001B[0;32m---> 31\u001B[0m display(df)\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/display.py:83\u001B[0m, in \u001B[0;36mDisplay.display\u001B[0;34m(self, input, *args, **kwargs)\u001B[0m\n\u001B[1;32m     80\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrigger\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m     81\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTriggers can only be set for streaming queries.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 83\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_custom_display_data\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m     86\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession\u001B[38;5;241m.\u001B[39mcreateDataFrame(\u001B[38;5;28minput\u001B[39m))\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/display.py:36\u001B[0m, in \u001B[0;36mDisplay.add_custom_display_data\u001B[0;34m(self, data_type, data)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21madd_custom_display_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, data_type, data):\n\u001B[1;32m     35\u001B[0m     custom_display_key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(uuid\u001B[38;5;241m.\u001B[39muuid4())\n\u001B[0;32m---> 36\u001B[0m     return_code \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mentry_point\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maddCustomDisplayData\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcustom_display_key\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m     ip_display({\n\u001B[1;32m     38\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/vnd.databricks.v1+display\u001B[39m\u001B[38;5;124m\"\u001B[39m: custom_display_key,\n\u001B[1;32m     39\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext/plain\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<Databricks Output (not supported in output widgets)>\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     40\u001B[0m     },\n\u001B[1;32m     41\u001B[0m                raw\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m return_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = \"\"\"\n",
    "{\n",
    "\"user_id\": 1,\n",
    "\"user_name\": \"John\",\n",
    "\"user_city\": \"London\"\n",
    "}\n",
    "{\n",
    "\"user_id\": 2,\n",
    "\"user_name\": \"Martin\",\n",
    "\"user_city\": \"New York\"\n",
    "}\n",
    "{\n",
    "\"user_id\": 3,\n",
    "\"user_name\": \"Sam\",\n",
    "\"user_city\": \"Sydney\"\n",
    "}\n",
    "{\n",
    "\"user_id\": 4,\n",
    "\"user_name\": \"Alan\",\n",
    "\"user_city\": \"Mexico City\"\n",
    "}\n",
    "{\n",
    "\"user_id\": 5,\n",
    "\"user_name\": \"Jacob\",\n",
    "\"user_city\": \"Florida\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "json_file_path_2 = \"/FileStore/tables/user_detail_multiline.json\"\n",
    "df = spark.read.json(path=json_file_path_2)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96ea37a5-fab6-4b5b-af56-c7999dbe3d8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "we can provide multiLine=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c756030-e002-4b94-b2c1-b1e0909c4a4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>user_city</th><th>user_id</th><th>user_name</th></tr></thead><tbody><tr><td>London</td><td>1</td><td>John</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "London",
         1,
         "John"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "user_city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "user_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "user_name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.json(path=json_file_path_2, multiLine=True)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18e55126-5d8e-41e1-8ae6-55993c9ae359",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It just takes the first entry from json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0ff28b2-6590-43a7-a096-6ccb6f0e679b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>user_city</th><th>user_id</th><th>user_name</th></tr></thead><tbody><tr><td>London</td><td>1</td><td>John</td></tr><tr><td>New York</td><td>2</td><td>Martin</td></tr><tr><td>Sydney</td><td>3</td><td>Sam</td></tr><tr><td>Mexico City</td><td>4</td><td>Alan</td></tr><tr><td>Florida</td><td>5</td><td>Jacob</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "London",
         1,
         "John"
        ],
        [
         "New York",
         2,
         "Martin"
        ],
        [
         "Sydney",
         3,
         "Sam"
        ],
        [
         "Mexico City",
         4,
         "Alan"
        ],
        [
         "Florida",
         5,
         "Jacob"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "user_city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "user_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "user_name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = \"\"\"\n",
    "[\n",
    "  {\n",
    "    \"user_id\": 1,\n",
    "    \"user_name\": \"John\",\n",
    "    \"user_city\": \"London\"\n",
    "  },\n",
    "  {\n",
    "    \"user_id\": 2,\n",
    "    \"user_name\": \"Martin\",\n",
    "    \"user_city\": \"New York\"\n",
    "  },\n",
    "  {\n",
    "    \"user_id\": 3,\n",
    "    \"user_name\": \"Sam\",\n",
    "    \"user_city\": \"Sydney\"\n",
    "  },\n",
    "  {\n",
    "    \"user_id\": 4,\n",
    "    \"user_name\": \"Alan\",\n",
    "    \"user_city\": \"Mexico City\"\n",
    "  },\n",
    "  {\n",
    "    \"user_id\": 5,\n",
    "    \"user_name\": \"Jacob\",\n",
    "    \"user_city\": \"Florida\"\n",
    "  }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "json_file_path_3 = \"/FileStore/tables/user_detail_multiline_in_list.json\"\n",
    "df = spark.read.json(path=json_file_path_3, multiLine=True)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd3be705-4109-4819-9538-1d96c546dd75",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can also provide schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "830b810f-8b32-430f-b08f-94ebf5ab9a1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>user_id</th><th>user_name</th><th>user_city</th></tr></thead><tbody><tr><td>1</td><td>John</td><td>London</td></tr><tr><td>2</td><td>Martin</td><td>New York</td></tr><tr><td>3</td><td>Sam</td><td>Sydney</td></tr><tr><td>4</td><td>Alan</td><td>Mexico City</td></tr><tr><td>5</td><td>Jacob</td><td>Florida</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "John",
         "London"
        ],
        [
         2,
         "Martin",
         "New York"
        ],
        [
         3,
         "Sam",
         "Sydney"
        ],
        [
         4,
         "Alan",
         "Mexico City"
        ],
        [
         5,
         "Jacob",
         "Florida"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "user_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "user_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "user_city",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "user_schema = StructType([\n",
    "                     StructField(\"user_id\", IntegerType(), True),\n",
    "                     StructField(\"user_name\", StringType(), True),\n",
    "                     StructField(\"user_city\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.json(path=json_file_path_3, multiLine=True, schema=user_schema)\n",
    "\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2. Json, Parquet",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
